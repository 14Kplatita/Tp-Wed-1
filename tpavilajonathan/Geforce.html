<!DOCTYPE HTML>
<html>
    <head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" href="main.css">
		<title>Geforce</title>
		
    </head>
        <body>
			
				<div class="pagina">
			<H1> GeForce </H1>
			
			<p>GeForce es una marca de tarjetas gráficas diseñadas por Nvidia desde 1999. Los primeros modelos fueron tarjetas de expansión para computadores de escritorio destinados a ejecutar gráficos para videojuegos, aunque más tarde sacaron modelos para computadores portátiles. Su principal competidor desde 2003 son las tarjetas gráficas Radeon diseñadas por ATI, hasta que AMD compró la empresa en 2006.</p>

			<p>En septiembre de 2020 lanzaron la serie 30, que varios fabricantes como ASUS, EVGA, GIGABYTE, MSI, Zotac venden como OEM. Esta generación destaca por tener núcleos dedicados a la ejecución de modelos de inteligencia artificial y trazado de rayos autodenominados «Tensor Cores» y «RT cores».</p>
			
			   
			
			  <p class="Indice">Indice</p>
				<ul class="lista de indice">
					<li><a href="#GeForce 256">GeForce 256</a></li>
					<li><a href="#Geforce 2">Geforce 2</a></li>
					<li><a href="#Geforce 3">Geforce 3</a></li>
                    <li><a href="#Geforce 4">Geforce 4</a></li>
                    <li><a href="#Geforce FX">Geforce FX</a></li>
				 </ul>
			
			
			<h2> Generaciones de GeForce </h2>
			  <hr>
			 
			  
					<h3 id="GeForce 256">GeForce 256</h3>		
			  <div class="amigo">   
			<img src="geforce/250px-VisionTek_GeForce_256.jpg" alt="VisionTek GeForce 256" >
 
            <ul>
                <li> GeForce 256 (32/64 MB SDR) </li>
                <li> GeForce 256 (32/64 MB DDR)> </li>
            </ul>     
            <p>La GeForce 256 tuvo un relativo éxito, eran consideradas tarjetas gráficas caras, pensadas para un usuario exigente o como tarjeta de desarrollo profesional barata. Su máximo competidor fue el primer procesador Radeon de ATI.</p>
            
            <table>
                    <tbody><tr>
                    <td><b>Fecha de lanzamiento</b>
                    </td>
                    <td>Octubre de 1999   
                    </td></tr>
                    <tr>
                    <td><b>Versión DirectX</b>
                    </td>
                    <td>7.0
                    </td></tr>
                    <tr>
                    <td><b>Número de transistores</b>
                    </td>
                    <td>23 millones
                    </td></tr>
                    </tbody>
            </table>
                    <p><b>Características destacadas:</b>
                    </p>
                        <ul>
                            <li>Procesador de 256 bits</li>
                            <li>Aparece el GPU</li>
                            <li>Soporte hardware deTransform Lighting</li>
                            <li>Soporte de Cube Environment Mapping (cube mapping)</li><li>Implementación de memoria DDR</li>
                        </ul>
				
				</div> 
 
			  <h3 id="Geforce 2"> GeForce 2 </h3>
			  <div class="amigo"> 
				  
			<img src="geforce/NVidia_GeForce2_MX400.jpg" alt="Tarjeta con el chip NVIDIA GeForce 2 MX 400"/>
				  
            <ul>
                <li> GeForce2 MX 100 (32 MB SDR) </li>
                <li> GeForce2 MX 200 (32/64 MB SDR/DDR) </li>
                <li> GeForce2 MX (32 MB SDR/DDR) </li>
                <li> GeForce2 MX 400 (32/64 MB SDR) </li>
                <li> GeForce2 2 GTS (32/64 MB SDR/DDR) </li>
                <li> GeForce2 2 PRO (32/64 MB SDR/DDR) </li>
                <li> GeForce2 2 TI VX (64 MB SDR/DDR) </li>
                <li> GeForce2 2 TI (64 MB SDR/DDR) </li>
                <li> GeForce2 2 ULTRA (32/64 MB SDR/DDR) </li>
            </ul>
                <p>La segunda generación del procesador NVIDIA vino marcada por un enorme éxito comercial y tecnológico. El GeForce 2 fue el procesador doméstico de gráficos más potente de su tiempo desbancando efectivamente a la competencia. La serie MX, de bajo coste, está entre las tarjetas gráficas más vendidas de la historia. Una versión prematura de GPU para ordenadores portátiles, el GeForce 2 Go, señaló la introducción de NVIDIA en este sector.</p>
			  
			  <table border="1">
                    <tbody><tr>
                    <td><b>Fecha de lanzamiento</b>
                    </td>
                    <td>abril de 2000   
                    </td></tr>
                    <tr>
                    <td><b>Versión DirectX</b>
                    </td>
                    <td>7.0
                    </td></tr>
                    <tr>
                    <td><b>Número de transistores</b>
                    </td>
                    <td>60 millones
                    </td></tr>
                    </tbody>
			  </table>
			  <p><b>Características destacadas:</b>
                    </p>
                        <ul>
                            <li>Motores independientes de hardware Transform & Lighting de segunda generación</li>
                            <li>Rasterizador shading integrado (Considerado una versión "antigua" de los shaders actuales)</li>
                            <li>Procesador de vídeo de alta definición integrado (HDTV)</li>
                            <li>Controlador de memoria DDR</li>
                        </ul>
			  </div>
			  
			  <h3 id="Geforce 3">Geforce 3</h3>  
			  <div class="amigo">
			  <img src="geforce/GeForce3_Ti_500.jpg">
			  
			  <ul>
                <li> GeForce3 (64/128 MB DDR </li>
                <li> GeForce3 Ti 200 (64/128 MB DDR) </li>
                <li> GeForce3 Ti 500 (64/128 MB DDR) </li>  
            </ul>
			  
			  <p>El procesador GeForce 3 fue lanzado prácticamente sin competencia real, ya que por parte de las compañías rivales no había un producto con características similares, siendo siempre un producto de gama alta del que nunca se desarrolló una versión económica. Asimismo, se trata del primer GPU programable con implementación nativa a la primera versión de DirectX 8. La consola Xbox presentó una implementación de este procesador para su soporte gráfico llamado NV2A, idéntico al GeForce 3 Ti500 pero con 2 unidades de proceso vertex paralelas en lugar de una.</p>
			  
			  <table border="1">
                    <tbody><tr>
                    <td><b>Fecha de lanzamiento</b>
                    </td>
                    <td>febrero de 2001  
                    </td></tr>
                    <tr>
                    <td><b>Versión DirectX</b>
                    </td>
                    <td>8.0
                    </td></tr>
                    <tr>
                    <td><b>Número de transistores</b>
                    </td>
                    <td>57 millones
                    </td></tr>
                    </tbody>
			  </table>
			  <p><b>Características destacadas:</b>
                    </p>
                        <ul>
                            <li>Vertex Shader y Pixel Shader programables</li>
                            <li>Optimización del bus de memoria (LightSpeed Memory Architecture)</li>
                            <li>Incorporación de multisampling antialiasing</li>
                        </ul>
			  </div>
			  
			  <h3 id="Geforce 4">Geforce 4</h3>
			  <div class="amigo">
			  <img src="geforce/200px-MSI_GeForce4_Ti_4800.png">
				  
			<ul>
                <li> GeForce4 MX 420 (64 MB SDR) </li>
                <li> GeForce4 MX 440SE (64 MB SDR/DDR) </li>
                <li> GeForce4 MX 440 8x (64/128 MB DDR) </li>
                <li> GeForce4 MX 460 (64 MB DDR) </li>
                <li> GeForce4 MX 4000 (64/128 MB DDR) </li>
				<li> GeForce PCX 4300 (128 MB DDR) </li>
                <li> GeForce4 Ti 4200 (64/128 MB DDR) </li>
                <li> GeForce4 Ti 4200 8x (128 MB DDR) </li>
				<li> GeForce4 Ti 4400 (128 MB DDR) </li>
				<li> GeForce4 Ti 4800 SE (128 MB DDR) </li>
                <li> GeForce4 Ti 4600 (128 MB DDR) </li>
                <li> GeForce4 Ti 4800 (128 MB DDR) </li>
            </ul>
				  
			<p>En la cuarta generación del procesador GeForce cabe distinguir entre la "auténtica" iteración, la familia GeForce 4 Ti de gama alta (alto rendimiento y desempeño) y la serie de bajo coste GeForce 4 MX (Lo único de GeForce 4 que tenía esta serie de placas era el nombre: eran nada más y nada menos que una GeForce 2 con algunos agregados como soporte AGP 8x, un controlador de memoria mejorado proveniente de la GeForce 4 real y un rudimentario procesador de vídeo, entre otros). La GeForce 4 Ti encontró rápidamente un hueco entre los usuarios de gráficos de alto rendimiento y fue extremadamente popular mientras que la serie MX, a pesar de su alargado éxito comercial, fue duramente criticada por la carencia de soporte PS/VS (al ser una GeForce 2 revitalizada) y por el uso abusivo del nombre comercial GeForce 4 que indujo a confusión a muchos usuarios. Se produjeron versiones para portátiles de la serie MX llamada GeForce 4 Go y una única adaptación que viera producción de la serie GeForce 4 Ti para portátiles, el GeForce 4200 Go.</p>
				  
			    <table border="1">
                    <tbody><tr>
                    <td><b>Fecha de lanzamiento</b>
                    </td>
                    <td>febrero de 2002 
                    </td></tr>
                    <tr>
                    <td><b>Versión DirectX</b>
                    </td>
                    <td>8.1
                    </td></tr>
                    <tr>
                    <td><b>Número de transistores</b>
                    </td>
                    <td>63 millones
                    </td></tr>
                    </tbody>
			  </table>
			  <p><b>Características destacadas:</b>
                    </p>
			  			<ul>
                            <li>2 unidades Vertex Shader programables (estructura de pipeline 4x2)</li>
                            <li>Pixel shader 1.3 (Por no soportar PS 1.4 no se considera una GPU compatible con DirectX 8.1)</li>
                            <li>Incorporación de multisampling antialiasing</li>
							<li>Soporte de visualización dual (NView)</li>
							<li>Controlador mejorado de memoria hasta 650 MHz DDR (LightSpeed Memory Architecture 2)</li>
							<li>Primera generación en soportar el bus AGP 8x (últimas versiones)</li>
                        </ul>
				  </div>
				  
			  <h3 id="Geforce FX">GeForce FX</h3>
			  <div class="amigo">
				  	<img src="geforce/250px-NVidia_GeForceFX_5500_SX.jpg">
					  
					<ul>
						<li> GeForce FX 5200(128/256 MB DDR) </li>
						<li> GeForce FX 5200 Ultra(256 MB DDR)</li>
						<li> GeForce PCX 5300(256 MB DDR) </li>
						<li> GeForce FX 5500(128/256 MB DDR) </li>
						<li> GeForce FX 5600 XT(128/256 MB DDR) </li>
						<li> GeForce FX 5600(64/128 MB DDR) </li>
						<li> GeForce FX 5600 Ultra(256 MB DDR) </li>
						<li> GeForce FX 5600 Ultra Rev.2(256 MB DDR) </li>
						<li> GeForce FX 5700 VE(128,256 MB DDR) </li>
						<li> GeForce FX 5700 LE(128,256 MB DDR) </li>
						<li> GeForce FX 5700(256 MB DDR) </li>
						<li> GeForce FX 5700 Ultra(256 MB DDR2) </li>
						<li> GeForce FX 5700 Ultra GDDR3(256 MB GDDR3) </li>
						<li> GeForce PCX 5750(256 MB GDDR3)</li>
						<li> GeForce FX 5800(256 MB DDR2) </li>
						<li> GeForce FX 5800 Ultra(256 MB DDR2) </li>
						<li> GeForce FX 5900 XT(256 MB DDR) </li>
						<li> GeForce FX 5900(128/256 MB DDR) </li>
						<li> GeForce FX 5900 Ultra(256 MB DDR) </li>
						<li> GeForce PCX 5900(256 MB DDR) </li>
						<li> GeForce FX 5950 Ultra(256 MB DDR) </li>
						<li> GeForce PCX 5950(256 MB DDR) </li>
					</ul>
				 <p>NVIDIA abandonó la tradicional nomenclatura de sus procesadores en favor del llamado motor FX, que decían iba a permitir a los usuarios de GeForce FX disfrutar de un avanzado motor de efectos y shaders programables. No obstante, desde las primeras muestras se comprobó que la gama alta de la serie FX rendía generalmente por debajo de su competidor la serie Radeon 9 de ATI en parte debido a fallos en los drivers y en parte a deficiencias en el diseño; y todo ello a pesar de haber salido al mercado seis meses más tarde.</p>
				  
				 <p>En la gama baja, las GeForce FX5200 / FX5600 rendían generalmente por alto de sus competidoras Radeon y, lo mejor, el rendimiento por píxel de la GeForce FX5200 resultaba inferior al de la tarjeta que supuestamente iba a sustituir, la serie GeForce4 MX basada en DirectX 7 y sin soporte de Pixel Shaders.</p>
				  
				 <p>Posteriormente, la introducción de las FX5700 y su equivalente Ultra trataron de enfrentarse a la potente serie Radeon 9600 de ATI, obteniendo resultados dispares. Mientras el rendimiento por pipeline de las FX5700 era técnicamente superior a su contrapartida Radeon, arrastraban todos los defectos de la deficiente implementación del lenguaje de shaders 2.0 de Microsoft que presentaba la serie FX.</p>
				  
				 <p>NVIDIA se adelantó al mercado desarrollando versiones de sus procesadores FX5200, FX5700 y FX5900 en el bus PCI-Express; para el que no existía entonces una gama de productos. Fueron respectivamente los modelos PCX5300, PCX5750 y PCX5900.</p>
				  
				 <p>Los importantes problemas de temperatura junto al enorme tamaño del microprocesador en sí impidieron a NVIDIA desarrollar una GPU para equipos portátiles basada en los procesadores más potentes de la serie FX, por lo que sólo existieron dos productos portátiles: GeForce FX5100 Go y FX5200 Go, ambos basados en el FX5200 de sobremesa.</p>
				  
				  <table border="1">
                    <tbody><tr>
                    <td><b>Fecha de lanzamiento</b>
                    </td>
                    <td>enero de 2003 
                    </td></tr>
                    <tr>
                    <td><b>Versión DirectX</b>
                    </td>
                    <td>	9.0 / 9.0b
                    </td></tr>
                    <tr>
                    <td><b>Número de transistores</b>
                    </td>
                    <td>aprox. 125 millones
                    </td></tr>
                    </tbody>
			  </table>
			  <p><b>Características destacadas:</b>
                    </p>
			  			<ul>
                            <li>Primer procesador en usar memoria DDR 2 (posteriormente hasta GDDR 3))</li>
                            <li>Soporte Vertex Shader 2.0+ y Pixel Shader 2.0+</li>
                            <li>Codificador de TV integrado</li>
							<li>Implementación mejorada de compresión de texturas y técnicas-Z</li>
							<li>Hasta 16 texturas por píxel</li>
                        </ul>
				  
				  </div>
			  	
			  		
			  </div>
              
      
			
			<script type="text/javascript" src="main.js"></script>
	</body>
    
</html>